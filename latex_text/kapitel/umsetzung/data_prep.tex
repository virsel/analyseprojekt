\newpage
\section{Methodik}
Die Umsetzung erfolgt in logischen Schritten und orientiert sich dabei am Prozessmodell \ac{CRISP-DM}.  

\subsection{Datenbeschaffung}\label{sec:data_ingestion}
Aufgrund dessen, dass Aktienprognosen mit multimodalen Daten ein sehr belebtes Forschungsfeld sind, gibt es bereits viele sinnvoll zusammengestellte Datensätze. Um diesen Vorteil auszuschöpfen, wird ein anerkannter Finanzdatensatz gewählt, welcher in mehreren Wissenschaftsarbeiten zum Einsatz kam \autocite{xu2018StockMovement}\autocite{Xu2020StockMovement}\autocite{zhang2022transformer}.

Der Datensatz umfasst Kursdaten und Tweets zu 88 Aktien, wobei jeweils eine Rohfassung und eine vorverarbeitete Variante bereitgestellt wird. In dieser Arbeit wird in beiden Fällen die Rohfassung verwendet. 

Vom Initial-Beschaffer wird angegeben, dass sich die Daten auf den Zeitraum 01.01.2014 bis 01.01.2016 beziehen \autocite[Kap. 3]{xu2018StockMovement}.
Es gibt jedoch auch Abweichungen, wie im Fall der Aktie ''BABA'', bei der Kursdaten für den Bereich 19.09.2014 bis 09.01.2017 vorliegen.

Im Rahmen dieser Arbeit werden lediglich Aktien der Branche Technologie einbezogen (Kap. \ref{sec:theorie_data_understanding}), welche im entsprechenden Zeitraum unter den Top-10-Aktien nach Handelsvolumen vorkommen. 

\subsubsection*{Kursdaten}\label{sec:data_ingestion_stockdata}
Zu jeder Aktie liegt eine \ac{CSV}-Datei vor. In Abbildung \ref{fig:goog_kursdaten} werden die inkludierten Metriken dargestellt.
\newpage
\begin{figure}[H]
	\includegraphics[width=1.\textwidth]{goog_kursdaten}
	\caption[Auszug aus GOOG-Kursdaten]{Auszug aus GOOG-Kursdaten. Quelle: \autocite{website:stocknet-dataset}}
	\label{fig:goog_kursdaten}
\end{figure}
 

\subsubsection*{Tweets}\label{sec:data_ingestion_tweetdata}
Zu jeder Aktie liegt für jeden Tag im jeweiligen Zeitraum eine Datei mit einer Tweet-Liste vor. Abbildung \ref{fig:goog_tweet} stellt einen Eintrag dieser Liste dar, wobei lediglich relevante Attribute einbezogen sind.
\begin{figure}[H]
	\includegraphics[width=1.\textwidth]{goog_tweet}
	\caption[GOOG Tweet vom 01.01.2014]{GOOG Tweet vom 01.01.2014. Quelle: \autocite{website:stocknet-dataset}}
	\label{fig:goog_tweet}
\end{figure}


\subsection{Datenvorverarbeitung}\label{sec:data_prep}

Die Datenvorverarbeitung erfolgt logisch in $4$ Teilschritten und wird mit der Python-Bibliothek \texttt{pandas} durchgeführt (Kap. \ref{sec:datenvorbereitung_pandas}). Während des 1. Arbeitsschritts werden die Daten aller Aktien in einer \ac{CSV}-Datei zusammengefasst, wobei zur Unterscheidung eine zusätzliche Datenspalte mit Marktkürzel als Füllwerte hinzugefügt wird (Abb. \ref{fig:data_prep_step1}, Spalte ''stock'').

\subsubsection*{1. Zeitliche Ausrichtung}

In diesem Arbeitsschritt werden pro Aktie Kursdaten und Tweets zu einer \ac{CSV}-Datei zusammengefasst. Dabei werden zunächst alle Kursdaten-Einträge entfernt, deren Erstellungsdatum außerhalb des Zeitraums liegt, für den Tweets vorliegen. Da diese Abweichung auch entgegengesetzt auftreten kann, werden auch alle Kurznachrichten gelöscht, deren Erstellungsdatum sich außerhalb des Zeitraums der Kursdaten befindet. Abbildung \ref{fig:data_prep_step1} zeigt eine resultierende Datenzeile.
\begin{figure}[H]
	\includegraphics[width=1.\textwidth]{data_prep_step1}
	\caption{GOOG-Datenzeile mit vereinigten Kurs- und Tweetdaten}
	\label{fig:data_prep_step1}
\end{figure}
Alle Tweets, die außerhalb von Markttagen entstanden sind, werden dem nächstmöglichen Markttag zugeordnet. Somit enthält die Zeile aus Abbildung \ref{fig:data_prep_step1} auch Kurznachrichten vom 04.01 und 05.01.2014. 


\subsubsection*{2. Textbereinigung}
Um die Tweets für eine Weiterverarbeitung durch ein großes Sprachmodell vorzubereiten, werden mehrere Bereinigungen durchgeführt. Dabei wird darauf geachtet, dass keinerlei semantische Informationen verloren gehen. Stopp-Wörter wie beispielsweise ''und'' werden daher nicht entfernt.
In Abbildung \ref{fig:data_prep_step1} ist zu erkennen, dass in Tweets häufig URLs eingebunden sind. Diese werden mit ''URL'' maskiert.
Des Weiteren treten regelmäßig Referenzierungen mit Hilfe von ''@'' auf. Diese stellen einen Verweis auf Entitäten wie beispielsweise Personen oder Unternehmen dar und werden mit der Maskierung ''AT\_ENTITY'' ersetzt. 
Außerdem erfolgt eine Substituierung von Aktienvorkommen der Form ''\$GOOG'' mit ''GOOG stock''. Denn es wird angenommen, dass für ein Sprachmodell die Semantik des Zeichens ''\$'' in diesem Zusammenhang weniger klar ist als die des Begriffs ''stock''. In Abbildung \ref{fig:data_prep_step2} ist die bereinigte Form der Tweets-Spalte aus Abbildung \ref{fig:data_prep_step1} zu sehen.
\begin{figure}[H]
	\includegraphics[width=1.\textwidth]{data_prep_step2}
	\caption{GOOG-Datenzeile nach der Bereinigung von Tweets}
	\label{fig:data_prep_step2}
\end{figure}


\subsubsection*{3. Stimmungsmetriken}

Dieser Arbeitsschritt sieht vor, dass pro Datenzeile anhand der enthaltenen Tweet-Liste Stimmungsmetriken extrahiert werden. Zunächst erfolgt eine Ableitung der Relevanz einer Aktie pro Tag, quantifiziert als Anzahl veröffentlichter Tweets (Code \ref{lst:feature_stock_relevanz}).
\begin{lstlisting}[caption={Relevanz quantifiziert als Anzahl-Tweets}, label=lst:feature_stock_relevanz, captionpos=t]
df['num_tweets'] = df['tweets'].apply(len)
\end{lstlisting}

Anschließend wird für jede Tweet-Liste eine durchschnittliche Quantifizierung der Metriken ''positiv'' und ''negative'' berechnet. Hierfür wird das vortrainierte Sprachmodell ''FinancialBERT'' eingesetzt (Kap. \ref{sec:data_prep_finbert}). Die genaue Funktionsweise ist im Code \ref{lst:feature_sent} beschrieben.
\begin{lstlisting}[caption={Stimmung quantifiziert als Positiv/Negativ-Score}, label=lst:feature_sent, captionpos=t]
# Initialisierung des Sprachmodells
model = BertForSequenceClassification.from_pretrained("<Pfad>")
tokenizer = BertTokenizer.from_pretrained("<Pfad>")

# Initialisierung der Klassifikations-Pipeline
nlp = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# Funktion zur Extraktion von Sentimentmetriken über eine Tweet-Liste
def comp_sent(texts):
	sent_res = []
	for text in texts:
		res = nlp(text)
		# Klassifikationsergebnis für einen Text
		# Format: [{'label': <Wert>, 'score': <Wert>}]
		sentiment = res[0]
		
		# Umwandlung zu Format: [positive_score, negative_score]
		res_formatted = [float(sentiment['label'] == 'positive') * sentiment['score'], 
		float(sentiment['label'] == 'negative') * sentiment['score']
		]
		# Hinzufügen des Resultats zur Liste
		sent_res.append(res_formatted)

	# Berechnung des Durchschnitts der "Tuple"
	res = list(np.mean(sent_res, axis=0)) if len(sent_res) > 0 else [0, 0]

	# Rückgabe des Durchschnitts über alle Tweets an diesem Tag als Dictionary
	return {
		'positive': res[0],
		'negative': res[1]
	}
\end{lstlisting}
Das Ergebnis dieser Operationen wird strukturiert als Spalten \texttt{num\_tweets}, \texttt{positive} und \texttt{negative} in den Datenkorpus eingefügt (Abb. \ref{fig:data_prep_step4}).


\subsubsection*{4. Stimmung-Embeddings}
Zusätzlich zu Stimmungsmetriken soll pro Tweet-Liste ein repräsentativer Embedding-Vektor erzeugt werden. Es wird angenommen, dass dadurch für ein neuronales Netz detaillierte Informationen bezüglich Tagesstimmung bereitgestellt sind. Hierfür werden pro Tag alle gesammelten Tweets einer Aktie verbunden und anschließend zur Berechnung des Embeddings an das FinancialBERT-Modell übergeben.
Dazu werden zunächst alle Tweets zu einem Text zusammengefasst, wobei das Sonderzeichen ''[SEP]'' als Bindeglied dient. Anschließend wird dem String das Zeichen ''[CLS]'' vorangestellt. Dies ist eine, seitens des zugrundeliegenden ''BERT''-Modells, empfohlene Vorgehensweise für Klassifikationsaufgaben, weshalb das Folgemodell ''FinancialBERT'' auch mit dieser Methode optimiert wurde \autocite[Kap. 5.3]{hazourli2022financialbert}.
Anschließend wird der kombinierte Text zu einer Token-ID-Liste umgewandelt (Code \ref{lst:feature_tweettokens}). 
\begin{lstlisting}[caption={Umwandlung von Tweets zu Token-IDs}, label=lst:feature_tweettokens, captionpos=t]
# Verbinden der Tweets mit Special-Token zu einem Text
text = ' [SEP] '.join(tweet_list)
# Klassifikations-Sonderzeichen voranstellen
text = '[CLS] ' + text

# Text zu Token-IDs umwandeln
inputs = tokenizer(
text, 
padding=True, 
truncation=True, 
return_tensors="pt"
)
\end{lstlisting}
Die Argumente \texttt{padding} und \texttt{truncation} in Zeile $8-9$ von Code \ref{lst:feature_tweettokens} stellen sicher, dass falls das Resultat die maximale Kontextlänge von $512$ Token unter- respektive überschreitet, entweder mit einem Sonderzeichen aufgefüllt wird oder ein entsprechendes Zuschneiden auf 512 IDs erfolgt.
Im letzten Schritt werden die Tokens an das Sprachmodell übergeben, welches mit berechneten Embeddings antwortet.
Während des iterativen \ac{CRISP-DM}-Prozesses hat sich herausgestellt, dass ein Zuschneiden der IDs bei Längenüberschreitung für eingesetzte neuronale Netzwerke (Kap. \ref{sec:modellierung}) besser funktioniert als das Aufteilen in Chunks mit anschließender Durchschnittsberechnung der Teil-Embeddings.

\subsubsection*{Ergebnis}
Der resultierende Datenkorpus ist im \ac{CSV}-Format gespeichert und enthält eine Kombination aus Kurs- und Stimmungsdaten von 10 Aktien verteilt auf $5470$ Zeilen (\ref{fig:data_prep_step4}).
\begin{figure}[H]
	\includegraphics[width=1.\textwidth]{data_prep_step4}
	\caption{GOOG-Datenzeile im Datenkorpus}
	\label{fig:data_prep_step4}
\end{figure}




