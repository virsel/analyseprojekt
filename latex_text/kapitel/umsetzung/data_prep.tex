\newpage
\section{Methodik}
Die Umsetzung erfolgt in logischen Schritten und orientiert sich dabei am Prozessmodell \ac{CRISP-DM}.  

\subsection{Datenbeschaffung}\label{sec:data_ingestion}
Aufgrund dessen, dass Aktienprognosen mit multimodalen Daten ein sehr belebtes Forschungsfeld ist, gibt es bereits viele sinnvoll zusammengestellte Datensätze. Um diesen Vorteil auszuschöpfen wird ein anerkannter Finanzdatensatz gewählt, welcher in mehreren Wissenschaftsarbeiten zum Einsatz kam \footcite{xu2018StockMovement}\footcite{Xu2020StockMovement}\footcite{zhang2022transformer}. Die Daten werden auf GitHub unter der MIT-Lizenz zur Verfügung gestellt \footcite{website:stocknet-dataset}. 

Der Datensatz umfasst Kursdaten und Tweets zu 88 Aktien, wobei jeweils eine Rohfassung und eine vorverarbeitete Variante bereitgestellt wird. In dieser Arbeit wird in beiden Fällen die Rohfassung verwendet. 

Vom Initial-Beschaffer wird angegeben, dass sich die Daten auf den Zeitraum 01.01.2014 bis 01.01.2016 beziehen \footcite[Kap. 3]{xu2018StockMovement}.
Es gibt jedoch auch Abweichungen, wie im Fall der Aktie ''BABA'', bei der Kursdaten für den Bereich 19.09.2014 bis 09.01.2017 vorliegen.

Im Rahmen dieser Arbeit werden lediglich Aktien der Branche Technologie einbezogen (Kap. \ref{sec:theorie_data_understanding}), welche im entsprechenden Zeitraum unter den Top 10 Aktien nach Handelsvolumen vorkommen. 

\subsubsection*{Kursdaten}\label{sec:data_ingestion_stockdata}
Zu jeder Aktie liegt eine \texttt{csv}-Datei vor. In Abbildung \ref{fig:goog_kursdaten} werden die inkludierten Metriken dargestellt.
\newpage
\begin{figure}[H]
	\caption{GOOG Kursdaten}
	\includegraphics[width=1.\textwidth]{goog_kursdaten}
	\label{fig:goog_kursdaten}
	\raggedright
	\normalsize{Quelle: Eigene Darstellung}
	\vspace{-1.0em}
\end{figure}
 

\subsubsection*{Tweets}\label{sec:data_ingestion_tweetdata}
Zu jeder Aktie liegt für jeden Tag im jeweiligen Zeitraum eine Datei mit einer Tweet-Liste vor. Abbildung \ref{fig:goog_tweet} stellt einen Eintrag dieser Liste dar, wobei lediglich relevante Attribute einbezogen werden.
\begin{figure}[H]
	\caption{GOOG Tweet vom 01.01.2014}
	\includegraphics[width=1.\textwidth]{goog_tweet}
	\label{fig:goog_tweet}
	\raggedright
	\normalsize{Quelle: Eigene Darstellung}
	\vspace{-1.0em}
\end{figure}


\subsection{Datenvorverarbeitung}\label{sec:data_prep}

Die Datenvorverarbeitung erfolgt logisch in $4$ Teilschritten. Während des 1. Arbeitsschritts werden die Daten aller Aktien in eine \texttt{csv}-Datei zusammengefasst, wobei zur Unterscheidung eine zusätzliche Datenspalte mit Marktkürzel als Werte hinzugefügt wird (Abb. \ref{fig:data_prep_step1}, Spalte ''stock'').

\subsubsection*{1. Zeitliche Ausrichtung der Daten}

In diesem Arbeitsschritt werden pro Aktie Kursdaten und Tweets zu einer \texttt{csv}-Datei zusammengefasst. Dabei werden zunächst alle Kursdaten-Einträge entfernt, dessen Erstellungsdatum außerhalb des Zeitraums liegt, für den Tweets vorliegen. Da diese Abweichung auch entgegengesetzt auftreten kann, werden auch alle Kurznachrichten gelöscht, deren Erstellungsdatum sich außerhalb des Bereichs der Kursdaten befindet. Abbildung \ref{fig:data_prep_step1} zeigt eine resultierende Datenzeile.
\begin{figure}[H]
	\caption{GOOG Datenzeile nach Schritt 1}
	\includegraphics[width=1.\textwidth]{data_prep_step1}
	\label{fig:data_prep_step1}
	\raggedright
	\normalsize{Quelle: Eigene Darstellung}
	\vspace{-1.0em}
\end{figure}
Alle Tweets die außerhalb von Markttagen entstanden sind, werden dem nächstmöglichen Markttag zugeordnet. Somit enthält die Zeile aus Abbildung \ref{fig:data_prep_step1} auch Kurznachrichten vom 04.01 und 05.01.2014. 


\subsubsection*{2. Textbereinigung}
Um die Tweets für eine Weiterverarbeitung durch ein großes Sprachmodell vorzubereiten, werden mehrere Bereinigungen durchgeführt. Dabei wird darauf geachtet, dass keinerlei symantische Informationen verloren gehen. Stopp-Wörter wie beispielsweise ''und'' werden daher nicht entfernt.
In Abbildung \ref{fig:data_prep_step1} ist zu erkennen, dass in Tweets häufig URL's eingebunden sind. Diese werden mit ''URL'' maskiert.
Des weiteren treten regelmäßig Referenzierungen mittel ''@'' auf. Diese werden mit der Maskierung ''AT\_ENTITY'' ersetzt. 
Außerdem erfolgt eine Substituierung von Aktienvorkommen der Form ''\$<Kürzel>'' mit ''<Kürzel> stock''. In Abbildung \ref{fig:data_prep_step2} ist die bereinigte Form der Tweets-Spalte aus Abbildung \ref{fig:data_prep_step1} zu sehen.
\begin{figure}[H]
	\caption{GOOG Datenzeile nach Schritt 2}
	\includegraphics[width=1.\textwidth]{data_prep_step2}
	\label{fig:data_prep_step2}
	\raggedright
	\normalsize{Quelle: Eigene Darstellung}
	\vspace{-1.0em}
\end{figure}


\subsubsection*{3. Stimmungsmetriken}

--
The maximum numbers of messages and words in a
single message are set to 30 and 40, respectively.
--\footcite[Kap. 5.2]{zhang2022transformer}

Dieser Arbeitsschritt sieht vor, dass pro Datenzeile anhand der enthaltenen Tweet-Liste Stimmungsmetriken extrahiert werden. Zunächst erfolgt eine Ableitung der Relevanz einer Aktie pro Tag quantifiziert als Anzahl veröffentlichter Tweets (Code \ref{lst:feature_stock_relevanz}).
\begin{lstlisting}[caption={Relevanz quantifiziert als Anzahl-Tweets}, label=lst:feature_stock_relevanz, captionpos=t]
df['num_tweets'] = df['tweets'].apply(len)
\end{lstlisting}
\vspace{-1.3em}
\normalsize{Quelle: Eigene Darstellung}

Anschließend wird für jede Tweet-Liste eine durchschnittliche Quantifizierung der Metriken ''positiv'' und ''negative'' berechnet. Hierfür wird das vortrainierte Sprachmodell ''FinancialBERT'' eingesetzt (Kap. \ref{sec:theorie_llm}). Die genaue Funktionsweise ist im Code \ref{lst:feature_sent} beschrieben.
\begin{lstlisting}[caption={Stimmung quantifiziert als Positiv/Negativ-Score}, label=lst:feature_sent, captionpos=t]
# Initialisierung des Sprachmodells
model = BertForSequenceClassification.from_pretrained("<Pfad>")
tokenizer = BertTokenizer.from_pretrained("<Pfad>")

# Initialisierung der Klassifikations-Pipeline
nlp = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# Funktion zur Extraktion von Sentiment Metriken über eine Tweet-Liste
def comp_sent(texts):
	sent_res = []
	for text in texts:
		res = nlp(text)
		# Klassifikation-Resultat für einen Text
		# Format: [{'label': <Wert>, 'score': <Wert>}]
		sentiment = res[0]
		
		# Umwandlung zu Format: [positive_score, negative_score]
		res_formatted = [float(sentiment['label'] == 'positive') * sentiment['score'], 
		float(sentiment['label'] == 'negative') * sentiment['score']
		]
		# Hinzufügen des Resultats zur Liste
		sent_res.append(res_formatted)

	# Berechnung des Durchschnitts der "Tuple"
	res = list(np.mean(sent_res, axis=0)) if len(sent_res) > 0 else [0, 0]

	# Rückgabe des Durchschnitts über alle Tweets an diesem Tag als Dictionary
	return {
		'positive': res[0],
		'negative': res[1]
	}
\end{lstlisting}
\vspace{-1.3em}
\normalsize{Quelle: Eigene Darstellung}


\subsubsection*{4. Stimmung-Embeddings}
Zusätzlich zu Stimmungsmetriken soll pro Tweet-Liste ein repräsentative Embedding-Vektor erzeugt werden. Es wird angenommen, dass dadurch für ein neuronales Netz detaillierte Informationen bezüglich Tagesstimmung bereitgestellt sind. Hierfür werden pro Tag alle gesammelten Tweets einer Aktie verbunden und anschließend zur Berechnung des Embeddings an das FinancialBERT-Modell übergeben. 
Dazu werden zunächst alle Tweets zu einem Text zusammengefasst wobei das Sonderzeichen ''[SEP]'' als Bindeglied dient. Anschließend wird dem String das Zeichen ''[CLS]'' vorangestellt. Dies ist eine, seitens des zugrundeliegenden ''BERT''-Modells, empfohlene Vorgehensweise für Klassifkationsaufgaben, weshalb das Folgemodell ''FinancialBERT'' auch mit dieser Methode optimiert wurde \footcite[Kap. 5.3]{hazourli2022financialbert}.
Anschließend wird der kombinierte Text zu einer Token-ID-Liste umgewandelt (Code \ref{lst:feature_tweettokens}). 
\begin{lstlisting}[caption={Umwandlung von Tweets zu Token-IDs}, label=lst:feature_tweettokens, captionpos=t]
	# Verbinden der Tweets mit Special-Token zu einem Text
	text = ' [SEP] '.join(tweet_list)
	# Klassifikations-Sonderzeichen voranstellen
	text = '[CLS] ' + text
	
	# Text zu Token-IDs umwandeln
	inputs = tokenizer(
	text, 
	padding=True, 
	truncation=True, 
	return_tensors="pt"
	)
\end{lstlisting}
\vspace{-1.3em}
\normalsize{Quelle: Eigene Darstellung}

Die Argumente \texttt{padding} und \texttt{truncation} in Zeile $8-9$ von Code \ref{lst:feature_tweettokens} stellen sicher, dass falls das Resultat die maximale Kontextlänge von $512$ Token unter- respektive überschreitet, entweder mit einem Sonderzeichen aufgefüllt wird oder ein entsprechendes Zuschneiden auf 512 IDs erfolgt.
Im letzten Schritt werden die Tokens an das Sprachmodell übergeben, welches mit berechneten Embeddings antwortet.
Während des iterativen \ac{CRISP-DM} Prozesses hat sich herausgestellt, dass ein Zuschneiden der ID's bei Längenüberschreitung für eingesetzte neuronale Netzwerke (Kap. \ref{sec:modellierung}) besser funktioniert, als das Aufteilen in Chunks mit anschließender Durchschnittsberechnung der Teil-Embeddings.

\subsubsection*{Ergebnis}
Der resultierende Datenkorpus ist im \texttt{csv}-Format gespeichert und enthält eine Kombination aus Kurs- und Stimmungsdaten von 10 Aktien verteilt auf $5470$ Zeilen (\ref{fig:data_prep_step4}).
\begin{figure}[H]
	\caption{GOOG Datenzeile im Datenkorpus}
	\includegraphics[width=1.\textwidth]{data_prep_step4}
	\label{fig:data_prep_step4}
	\raggedright
	\normalsize{Quelle: Eigene Darstellung}
	\vspace{-1.0em}
\end{figure}




